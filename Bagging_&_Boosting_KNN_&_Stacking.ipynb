{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the fundamental idea behind ensemble techniques? How does bagging differ from boosting in terms of approach and objective?\n",
        "\n",
        "=> Fundamental Idea: Ensemble techniques combine multiple models (often called \"base learners\") to improve predictive performance by reducing variance, bias, or improving predictions. The core principle is that a group of weak learners can together create a strong learner.\n",
        "\n",
        "Bagging vs. Boosting:\n",
        "\n",
        "Bagging (Bootstrap Aggregating): Builds multiple models independently on random subsets of the training data and aggregates their predictions (e.g., by majority voting for classification). Its main goal is to reduce variance and avoid overfitting.\n",
        "\n",
        "Boosting: Builds models sequentially, each correcting the errors of the previous one. It aims to reduce bias by focusing more on wrong predictions (hard cases) in each iteration.\n",
        "\n",
        "2. Explain how the Random Forest Classifier reduces overfitting compared to a single decision tree. Mention the role of two key hyperparameters in this process.\n",
        "\n",
        "=> Reduction of Overfitting: Random Forest creates an ensemble of decision trees using bootstrapped samples and random feature selection, making the overall model less sensitive to noise and overfitting compared to one tree.\n",
        "\n",
        "Two Key Hyperparameters:\n",
        "\n",
        "n_estimators (Number of Trees): More trees help average out errors and reduce variance.\n",
        "\n",
        "max_features (Number of Features per Split): Limits feature selection for each split, ensuring individual trees are less correlated, promoting diversity and robustness.\n",
        "\n",
        "3. What is Stacking in ensemble learning? How does it differ from traditional bagging/boosting methods? Provide a simple example use case.\n",
        "\n",
        "=> Stacking: Combines predictions from several diverse models (base learners) using another model (meta learner) that learns the optimal way to combine outputs.\n",
        "\n",
        "Difference: Unlike bagging/boosting, stacking does not just aggregate or sequentially adjust models; it uses a learner to intelligently combine predictions.\n",
        "\n",
        "Example: Predicting housing prices by stacking outputs from decision trees, linear regression, and SVM, then blending with a meta-model like logistic regression to get the final prediction.\n",
        "\n",
        "4. What is the OOB Score in Random Forest, and why is it useful? How does it help in model evaluation without a separate validation set?\n",
        "\n",
        "=> OOB Score (Out-of-Bag): Uses samples not included in a tree's bootstrap sample as validation data for that tree.\n",
        "\n",
        "Utility: Provides an unbiased estimate of model performance during training without needing a separate validation split, conserving data for training.\n",
        "\n",
        "5. Compare AdaBoost and Gradient Boosting:\n",
        "\n",
        "=> Handling Errors: AdaBoost increases weights for misclassified samples; Gradient Boosting fits the new model to the residuals (errors) of the previous model.\n",
        "\n",
        "Weight Adjustment: AdaBoost reweights training examples; Gradient Boosting uses gradients to minimize a specific loss function.\n",
        "\n",
        "Typical Use Cases: AdaBoost is often used for binary classification; Gradient Boosting is used in both regression and classification tasks due to its flexibility.\n",
        "\n",
        "6. Why does CatBoost perform well on categorical features without requiring extensive preprocessing? Briefly explain its handling of categorical variables.\n",
        "\n",
        "=> Reason: CatBoost automatically encodes categorical variables internally using techniques like ordered target statistics, reducing overfitting and leakage.\n",
        "\n",
        "Handling: It does not require manual encoding like one-hot; CatBoost transforms categories iteratively based on training order, preserving information and reducing preprocessing.\n",
        "\n",
        "7. KNN Classifier Assignment:\n",
        "\n",
        "=> Load Wine dataset with sklearn.datasets.load_wine.\n",
        "\n",
        "Split into 70% train, 30% test.\n",
        "\n",
        "Train KNN (K=5, no scaling) and evaluate accuracy, precision, recall, F1-Score (use classification_report).\n",
        "\n",
        "Apply StandardScaler, retrain KNN, compare metrics.\n",
        "\n",
        "Use GridSearchCV to find best K (1-20) and distance (Euclidean, Manhattan).\n",
        "\n",
        "Train optimized KNN and compare all results.\n",
        "\n",
        "8. PCA KNN with Variance Analysis and Visualization:\n",
        "\n",
        "=> Load Breast Cancer dataset (sklearn.datasets.load_breast_cancer).\n",
        "\n",
        "Apply PCA, plot explained variance (scree plot).\n",
        "\n",
        "Retain 95% variance, transform dataset.\n",
        "\n",
        "Train KNN on original and transformed data, compare accuracy.\n",
        "\n",
        "Visualize first two principal components (scatter plot colored by class).\n",
        "\n",
        "9. KNN Regressor with Distance Metrics and K-Value Analysis:\n",
        "\n",
        "=> Generate synthetic regression dataset (sklearn.datasets.make_regression, n_samples=500, n_features=10).\n",
        "\n",
        "Train KNN regressor:\n",
        "\n",
        "Euclidean, K=5.\n",
        "\n",
        "Manhattan, K=5.\n",
        "\n",
        "Compare MSE for both.\n",
        "\n",
        "Test K=, plot K vs MSE (analyze bias-variance).â€‹\n",
        "\n",
        "10. KNN with KD-Tree/Ball Tree, Imputation, and Real-World Data:\n",
        "\n",
        "=> Load Pima Indians Diabetes dataset (has missing values).\n",
        "\n",
        "Use KNN Imputation (sklearn.impute.KNNImputer).\n",
        "\n",
        "Train KNN using:\n",
        "\n",
        "Brute-force.\n",
        "\n",
        "KD-Tree.\n",
        "\n",
        "Ball Tree.\n",
        "\n",
        "Compare training time, accuracy.\n",
        "\n",
        "Plot decision boundary for best method (using 2 most important features).\n",
        "\n"
      ],
      "metadata": {
        "id": "Q44VzVJD1vSV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P4Pc-v5m1OHX"
      },
      "outputs": [],
      "source": []
    }
  ]
}