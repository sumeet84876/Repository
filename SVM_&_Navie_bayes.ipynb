{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "Information Gain measures how much “information” a feature gives us about the class. It represents the reduction in entropy or impurity after a dataset is split on a particular attribute. In decision trees, when building the tree, the algorithm selects the feature with the highest information gain to split the data at each node, as this gives the purest (least impure) split.\n",
        "\n",
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Gini Impurity measures the likelihood of incorrect classification of a randomly chosen element if it was randomly labeled according to the distribution of labels in the dataset.\n",
        "\n",
        "Entropy measures the amount of uncertainty or impurity in the dataset.\n",
        "Comparison:\n",
        "\n",
        "Gini is computationally simpler than entropy.\n",
        "\n",
        "Entropy has a stronger theoretical foundation linked to information theory.\n",
        "\n",
        "Both serve as criteria for splitting nodes in decision trees.\n",
        "\n",
        "Gini tends to create splits that isolate the most frequent class\n",
        "\n",
        "\n",
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Answer:\n",
        "Pre-Pruning is a method used to stop the growth of a decision tree early, before it perfectly fits the training data, to avoid overfitting. It sets conditions such as maximum tree depth, minimum samples required for a split, or minimum impurity decrease required, so branches are stopped before they become too specific."
      ],
      "metadata": {
        "id": "8QJTCwZFY-hh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical)."
      ],
      "metadata": {
        "id": "uqNaHs4nZVUn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eA7isRFVYkdA",
        "outputId": "19767910-038c-4f54-e4af-d50deb978ff3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature importances: [0.01333333 0.         0.56405596 0.42261071]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "X, y = load_iris(return_X_y=True)\n",
        "clf = DecisionTreeClassifier(criterion='gini')\n",
        "clf.fit(X, y)\n",
        "print(\"Feature importances:\", clf.feature_importances_)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Answer:\n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification or regression. It works by finding the hyperplane that best separates different classes in the feature space, maximizing the margin between the closest points of the classes (support vectors)."
      ],
      "metadata": {
        "id": "EtixJXYDZhiw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "Answer:\n",
        "The Kernel Trick is a method used by SVMs to solve non-linear problems by operating in a transformed feature space. By applying a kernel function, SVM can efficiently compute the separation boundary in a higher-dimensional space without explicitly performing the transformation."
      ],
      "metadata": {
        "id": "jjY7zZH2ZqWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Python program to train two SVM classifiers (Linear and RBF kernels) on the Wine dataset and compare accuracies"
      ],
      "metadata": {
        "id": "MITALGgqZu_y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "clf_linear = SVC(kernel='linear')\n",
        "clf_linear.fit(X_train, y_train)\n",
        "acc_linear = accuracy_score(y_test, clf_linear.predict(X_test))\n",
        "\n",
        "clf_rbf = SVC(kernel='rbf')\n",
        "clf_rbf.fit(X_train, y_train)\n",
        "acc_rbf = accuracy_score(y_test, clf_rbf.predict(X_test))\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", acc_linear)\n",
        "print(\"RBF Kernel Accuracy:\", acc_rbf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7PH4niOZcWG",
        "outputId": "fb4e9f44-7bba-4acc-f838-7d4a1506e8bd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9629629629629629\n",
            "RBF Kernel Accuracy: 0.6851851851851852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n",
        "\n",
        "Answer:\n",
        "The Naive Bayes classifier is a probabilistic machine learning model based on Bayes’ theorem, assuming all features are independent (“naive” assumption) given the class label. Its simplicity makes it effective and fast for many classification tasks."
      ],
      "metadata": {
        "id": "mdw7BtMMZ5h9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Differences between Gaussian, Multinomial, and Bernoulli Naive Bayes\n",
        "\n",
        "Answer:\n",
        "\n",
        "Gaussian Naive Bayes: Assumes features are continuous and normally distributed.\n",
        "\n",
        "Multinomial Naive Bayes: Assumes features are discrete counts (typically used for text).\n",
        "\n",
        "Bernoulli Naive Bayes: Assumes binary/boolean feature values."
      ],
      "metadata": {
        "id": "zqYYmqdlaG2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Naïve Bayes, Multinomial Naïve Bayes, and Bernoulli Naïve Bayes?\n",
        "\n",
        "Answer:\n",
        "\n",
        "Gaussian Naive Bayes: Assumes features are continuous and normally distributed.\n",
        "\n",
        "Multinomial Naive Bayes: Assumes features are discrete counts (typically used for text).\n",
        "\n",
        "Bernoulli Naive Bayes: Assumes binary/boolean feature values."
      ],
      "metadata": {
        "id": "7bxVdrH5aIRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets.\n"
      ],
      "metadata": {
        "id": "WNX7JBGNaZ4t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "clf = GaussianNB()\n",
        "clf.fit(X_train, y_train)\n",
        "acc = accuracy_score(y_test, clf.predict(X_test))\n",
        "\n",
        "print(\"Gaussian Naive Bayes Accuracy:\", acc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rx8cI4hsZ4AV",
        "outputId": "e2d00af6-d9f5-4a4e-e09a-9256d4f7cb44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naive Bayes Accuracy: 0.9532163742690059\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GtDhJyCsakV-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}